{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project - Text Classification\n",
    "### Text Processing and Naive Bayes\n",
    "\n",
    "**Dataset:**\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset/version/1\n",
    "\n",
    "**Objective:** to classify SMS message as spam or not spam (ham).\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "From the given data set, use Naïve Bayes to classify the SMS message.\n",
    "The framework for text classification is briefly summarized here:\n",
    "* Preprocessing of the dataset (change to lower case, remove numbers, remove punctuation, stop words, white space, word stemming, etc.)\n",
    "* Document-Term-Matrix creation – matrix of word counts for each individual document in the matrix (e.g. documents as rows, words as columns or vice versa)\n",
    "* Text Analysis (e.g. word counts, visualizations using wordclouds)\n",
    "* Predict Spam or Not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop empty ones\n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "df = df.dropna(how=\"any\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name the columns\n",
    "df.columns = ['target', 'message']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4825,  747], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out how many of each class there are\n",
    "balance_counts = df.groupby('target')['target'].agg('count').values\n",
    "balance_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is imbalanced.  There isn't much I can do about this, but it is important to note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham messages average characters: 71.02362694300518\n",
      "Spam messages average characters: 138.8661311914324\n"
     ]
    }
   ],
   "source": [
    "# find average length of message\n",
    "\n",
    "ham_lengths = []\n",
    "spam_lengths = []\n",
    "\n",
    "for x in range(len(df['message'])):\n",
    "    if df['target'][x] == 'ham':\n",
    "        ham_lengths.append(len(df['message'][x]))\n",
    "    else:\n",
    "        spam_lengths.append(len(df['message'][x]))\n",
    "\n",
    "avg_ham = sum(ham_lengths)/len(ham_lengths)\n",
    "avg_spam = sum(spam_lengths)/len(spam_lengths)\n",
    "\n",
    "print(\"Ham messages average characters:\",avg_ham)\n",
    "print(\"Spam messages average characters:\",avg_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see that spam messages have almost double the characters than ham messages.  Perhpas it is because real texts in the course of conversation could be only one or two words, while spam is usually trying to sell something or provide lots of info to get the reader to act on.  Spam is naturally longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['message'][0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham messages average words: 14.20062176165803\n",
      "Spam messages average words: 23.85140562248996\n"
     ]
    }
   ],
   "source": [
    "# find average length of message\n",
    "\n",
    "ham_words_count = []\n",
    "spam_words_count = []\n",
    "\n",
    "for x in range(len(df['message'])):\n",
    "    if df['target'][x] == 'ham':\n",
    "        ham_words_count.append(len(df['message'][x].split()))\n",
    "    else:\n",
    "        spam_words_count.append(len(df['message'][x].split()))\n",
    "\n",
    "avg_ham_words = sum(ham_words_count)/len(ham_words_count)\n",
    "avg_spam_words = sum(spam_words_count)/len(spam_words_count)\n",
    "\n",
    "print(\"Ham messages average words:\",avg_ham_words)\n",
    "print(\"Spam messages average words:\",avg_spam_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, spam messages are longer, this time measured by average words in messages. I'm guessing this is for the same reasoning as above.\n",
    "\n",
    "To get more insight, I will need to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text\n",
    "\n",
    "Cleaning the text makes it possible to input into a model.  It is important to standardize the data and remove the most common words that will not really help uncover any patterns because they are so prevalent in both classes.  Finally, word stemming acts as a normalization technique too.  Trimming the words to their roots makes different, but related words (aka ones with the same stems) appear appropriately as similar to the model.  \n",
    "\n",
    "- Make Lowercase\n",
    "- Remove Punctuation\n",
    "- Remove Numbers\n",
    "- Strip Whitespace\n",
    "- Remove Stopwords\n",
    "- Word Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       message_lower  \n",
       "0  go until jurong point, crazy.. available only ...  \n",
       "1                      ok lar... joking wif u oni...  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3  u dun say so early hor... u c already then say...  \n",
       "4  nah i don't think he goes to usf, he lives aro...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Messages All Lowercase\n",
    "df['message_lower'] = df['message'].str.lower()\n",
    "df.head()\n",
    "\n",
    "# Source: https://www.kite.com/python/answers/how-to-make-a-pandas-dataframe-string-column-lowercase-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_lower</th>\n",
       "      <th>message_no_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       message_lower  \\\n",
       "0  go until jurong point, crazy.. available only ...   \n",
       "1                      ok lar... joking wif u oni...   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3  u dun say so early hor... u c already then say...   \n",
       "4  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    message_no_punct  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Puntuation\n",
    "df['message_no_punct'] = df['message_lower'].str.replace(r'[^\\w\\s]+', '')\n",
    "df.head()\n",
    "\n",
    "#source = https://stackoverflow.com/questions/50444346/fast-punctuation-removal-with-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_lower</th>\n",
       "      <th>message_no_punct</th>\n",
       "      <th>message_no_nums</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       message_lower  \\\n",
       "0  go until jurong point, crazy.. available only ...   \n",
       "1                      ok lar... joking wif u oni...   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3  u dun say so early hor... u c already then say...   \n",
       "4  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    message_no_punct  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                     message_no_nums  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in  a wkly comp to win fa cup final...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Numbers\n",
    "df['message_no_nums'] = df['message_no_punct'].str.replace('\\d+', '')\n",
    "df.head()\n",
    "\n",
    "#source = https://stackoverflow.com/questions/41719259/how-to-remove-numbers-from-string-terms-in-a-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_lower</th>\n",
       "      <th>message_no_punct</th>\n",
       "      <th>message_no_nums</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       message_lower  \\\n",
       "0  go until jurong point, crazy.. available only ...   \n",
       "1                      ok lar... joking wif u oni...   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3  u dun say so early hor... u c already then say...   \n",
       "4  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    message_no_punct  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                     message_no_nums  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in  a wkly comp to win fa cup final...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strip White Space\n",
    "df['message_no_nums'].str.strip()\n",
    "df.head()\n",
    "\n",
    "#source = https://www.geeksforgeeks.org/pandas-strip-whitespace-from-entire-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_no_nums</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey there darling its been  weeks now ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>as per your request melle melle oru minnaminun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>winner as a valued network customer you have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>had your mobile  months or more u r entitled t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "5   spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "6    ham  Even my brother is not like to speak with me. ...   \n",
       "7    ham  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "8   spam  WINNER!! As a valued network customer you have...   \n",
       "9   spam  Had your mobile 11 months or more? U R entitle...   \n",
       "\n",
       "                                     message_no_nums  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in  a wkly comp to win fa cup final...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i dont think he goes to usf he lives aroun...  \n",
       "5  freemsg hey there darling its been  weeks now ...  \n",
       "6  even my brother is not like to speak with me t...  \n",
       "7  as per your request melle melle oru minnaminun...  \n",
       "8  winner as a valued network customer you have b...  \n",
       "9  had your mobile  months or more u r entitled t...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Uneeded Columns\n",
    "df.drop(['message_lower', 'message_no_punct'],axis=1, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_no_nums</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey there darling its been  weeks now ...</td>\n",
       "      <td>freemsg hey darling weeks word back id like fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>even brother like speak treat like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>as per your request melle melle oru minnaminun...</td>\n",
       "      <td>per request melle melle oru minnaminunginte nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>winner as a valued network customer you have b...</td>\n",
       "      <td>winner valued network customer selected receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>had your mobile  months or more u r entitled t...</td>\n",
       "      <td>mobile months u r entitled update latest colou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "      <td>im gonna be home soon and i dont want to talk ...</td>\n",
       "      <td>im gonna home soon dont want talk stuff anymor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "      <td>six chances to win cash from  to  pounds txt c...</td>\n",
       "      <td>six chances win cash pounds txt csh send cost ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "      <td>urgent you have won a  week free membership in...</td>\n",
       "      <td>urgent week free membership å prize jackpot tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>ive searching right words thank breather promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                            message  \\\n",
       "0     ham  Go until jurong point, crazy.. Available only ...   \n",
       "1     ham                      Ok lar... Joking wif u oni...   \n",
       "2    spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     ham  U dun say so early hor... U c already then say...   \n",
       "4     ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "5    spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "6     ham  Even my brother is not like to speak with me. ...   \n",
       "7     ham  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "8    spam  WINNER!! As a valued network customer you have...   \n",
       "9    spam  Had your mobile 11 months or more? U R entitle...   \n",
       "10    ham  I'm gonna be home soon and i don't want to tal...   \n",
       "11   spam  SIX chances to win CASH! From 100 to 20,000 po...   \n",
       "12   spam  URGENT! You have won a 1 week FREE membership ...   \n",
       "13    ham  I've been searching for the right words to tha...   \n",
       "14    ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                      message_no_nums  \\\n",
       "0   go until jurong point crazy available only in ...   \n",
       "1                             ok lar joking wif u oni   \n",
       "2   free entry in  a wkly comp to win fa cup final...   \n",
       "3         u dun say so early hor u c already then say   \n",
       "4   nah i dont think he goes to usf he lives aroun...   \n",
       "5   freemsg hey there darling its been  weeks now ...   \n",
       "6   even my brother is not like to speak with me t...   \n",
       "7   as per your request melle melle oru minnaminun...   \n",
       "8   winner as a valued network customer you have b...   \n",
       "9   had your mobile  months or more u r entitled t...   \n",
       "10  im gonna be home soon and i dont want to talk ...   \n",
       "11  six chances to win cash from  to  pounds txt c...   \n",
       "12  urgent you have won a  week free membership in...   \n",
       "13  ive been searching for the right words to than...   \n",
       "14                  i have a date on sunday with will   \n",
       "\n",
       "                                        clean_message  \n",
       "0   go jurong point crazy available bugis n great ...  \n",
       "1                             ok lar joking wif u oni  \n",
       "2   free entry wkly comp win fa cup final tkts st ...  \n",
       "3                 u dun say early hor u c already say  \n",
       "4         nah dont think goes usf lives around though  \n",
       "5   freemsg hey darling weeks word back id like fu...  \n",
       "6      even brother like speak treat like aids patent  \n",
       "7   per request melle melle oru minnaminunginte nu...  \n",
       "8   winner valued network customer selected receiv...  \n",
       "9   mobile months u r entitled update latest colou...  \n",
       "10  im gonna home soon dont want talk stuff anymor...  \n",
       "11  six chances win cash pounds txt csh send cost ...  \n",
       "12  urgent week free membership å prize jackpot tx...  \n",
       "13  ive searching right words thank breather promi...  \n",
       "14                                        date sunday  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "\n",
    "# Import stopwords with nltk.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['clean_message'] = df['message_no_nums'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df.head(15)\n",
    "\n",
    "#Source = https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, there are still some words remaining that would probably be classified as stopwords if they were spelled correctly or were normal characters.  Some examples include: \"ok\",\"u\",\"c\",\"r\",\"å\",\"id\".\n",
    "\n",
    "I will remove these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_no_nums</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>lar joking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>dun say early hor already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey there darling its been  weeks now ...</td>\n",
       "      <td>freemsg hey darling weeks word back like fun s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>even brother like speak treat like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>as per your request melle melle oru minnaminun...</td>\n",
       "      <td>per request melle melle oru minnaminunginte nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>winner as a valued network customer you have b...</td>\n",
       "      <td>winner valued network customer selected receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>had your mobile  months or more u r entitled t...</td>\n",
       "      <td>mobile months entitled update latest colour mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "      <td>im gonna be home soon and i dont want to talk ...</td>\n",
       "      <td>im gonna home soon dont want talk stuff anymor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "      <td>six chances to win cash from  to  pounds txt c...</td>\n",
       "      <td>six chances win cash pounds txt csh send cost ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "      <td>urgent you have won a  week free membership in...</td>\n",
       "      <td>urgent week free membership prize jackpot txt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>ive searching right words thank breather promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                            message  \\\n",
       "0     ham  Go until jurong point, crazy.. Available only ...   \n",
       "1     ham                      Ok lar... Joking wif u oni...   \n",
       "2    spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     ham  U dun say so early hor... U c already then say...   \n",
       "4     ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "5    spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "6     ham  Even my brother is not like to speak with me. ...   \n",
       "7     ham  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "8    spam  WINNER!! As a valued network customer you have...   \n",
       "9    spam  Had your mobile 11 months or more? U R entitle...   \n",
       "10    ham  I'm gonna be home soon and i don't want to tal...   \n",
       "11   spam  SIX chances to win CASH! From 100 to 20,000 po...   \n",
       "12   spam  URGENT! You have won a 1 week FREE membership ...   \n",
       "13    ham  I've been searching for the right words to tha...   \n",
       "14    ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                      message_no_nums  \\\n",
       "0   go until jurong point crazy available only in ...   \n",
       "1                             ok lar joking wif u oni   \n",
       "2   free entry in  a wkly comp to win fa cup final...   \n",
       "3         u dun say so early hor u c already then say   \n",
       "4   nah i dont think he goes to usf he lives aroun...   \n",
       "5   freemsg hey there darling its been  weeks now ...   \n",
       "6   even my brother is not like to speak with me t...   \n",
       "7   as per your request melle melle oru minnaminun...   \n",
       "8   winner as a valued network customer you have b...   \n",
       "9   had your mobile  months or more u r entitled t...   \n",
       "10  im gonna be home soon and i dont want to talk ...   \n",
       "11  six chances to win cash from  to  pounds txt c...   \n",
       "12  urgent you have won a  week free membership in...   \n",
       "13  ive been searching for the right words to than...   \n",
       "14                  i have a date on sunday with will   \n",
       "\n",
       "                                        clean_message  \n",
       "0   go jurong point crazy available bugis n great ...  \n",
       "1                                  lar joking wif oni  \n",
       "2   free entry wkly comp win fa cup final tkts st ...  \n",
       "3                       dun say early hor already say  \n",
       "4         nah dont think goes usf lives around though  \n",
       "5   freemsg hey darling weeks word back like fun s...  \n",
       "6      even brother like speak treat like aids patent  \n",
       "7   per request melle melle oru minnaminunginte nu...  \n",
       "8   winner valued network customer selected receiv...  \n",
       "9   mobile months entitled update latest colour mo...  \n",
       "10  im gonna home soon dont want talk stuff anymor...  \n",
       "11  six chances win cash pounds txt csh send cost ...  \n",
       "12  urgent week free membership prize jackpot txt ...  \n",
       "13  ive searching right words thank breather promi...  \n",
       "14                                        date sunday  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove More Stopwords\n",
    "\n",
    "# Add more stopwords\n",
    "extra_words = [\"ok\",\"u\",\"c\",\"r\",\"å\",\"id\"]\n",
    "stop_words = stop + extra_words\n",
    "\n",
    "# Remove stopwords\n",
    "df['clean_message'] = df['message_no_nums'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>lar joking wif oni</td>\n",
       "      <td>lar joke wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "      <td>free entri wkli comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>dun say early hor already say</td>\n",
       "      <td>dun say earli hor alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       clean_message  \\\n",
       "0  go jurong point crazy available bugis n great ...   \n",
       "1                                 lar joking wif oni   \n",
       "2  free entry wkly comp win fa cup final tkts st ...   \n",
       "3                      dun say early hor already say   \n",
       "4        nah dont think goes usf lives around though   \n",
       "\n",
       "                                             stemmed  \n",
       "0  go jurong point crazi avail bugi n great world...  \n",
       "1                                   lar joke wif oni  \n",
       "2  free entri wkli comp win fa cup final tkts st ...  \n",
       "3                      dun say earli hor alreadi say  \n",
       "4          nah dont think goe usf live around though  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Stemming\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Drop Uneeded Columns\n",
    "df.drop(['message_no_nums'],axis=1, inplace=True)\n",
    "\n",
    "# Use English stemmer.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Stem every word\n",
    "df['stemmed'] = df['clean_message'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df.head()\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/37443138/python-stemming-with-pandas-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term-Matrix creation\n",
    "\n",
    "Steps:\n",
    "- First, need to make the labels binary.  'ham' and 'spam' will work better to classify as 0 and 1\n",
    "- Split into train test sets\n",
    "- Will fit the vectorize the train set only (because it would give the model an undue advantage to vectorize on the test set too).\n",
    "- Vectorize the train and test set, based on the fit from the train set. \n",
    "- TF-IDF\n",
    "\n",
    "Then the text will be ready to be inputted and used to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>b_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>lar joking wif oni</td>\n",
       "      <td>lar joke wif oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "      <td>free entri wkli comp win fa cup final tkts st ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>dun say early hor already say</td>\n",
       "      <td>dun say earli hor alreadi say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       clean_message  \\\n",
       "0  go jurong point crazy available bugis n great ...   \n",
       "1                                 lar joking wif oni   \n",
       "2  free entry wkly comp win fa cup final tkts st ...   \n",
       "3                      dun say early hor already say   \n",
       "4        nah dont think goes usf lives around though   \n",
       "\n",
       "                                             stemmed  b_target  \n",
       "0  go jurong point crazi avail bugi n great world...         0  \n",
       "1                                   lar joke wif oni         0  \n",
       "2  free entri wkli comp win fa cup final tkts st ...         1  \n",
       "3                      dun say earli hor alreadi say         0  \n",
       "4          nah dont think goe usf live around though         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the target from 'ham' and 'spam' to 0 and 1\n",
    "df['b_target'] = df['target'].map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X (predictive features) and Y (target feature)\n",
    "X = df['stemmed']\n",
    "Y = df['b_target']\n",
    "\n",
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "Vectorization is a key step in data preparation.  Vectorization creates a matrix with every word in the dataset as a column.  The rows of the matrix are each message in the data, with 1s in the column for the word included in the message. This helps 'quanitfy' the text data in a way that the model can analyze.\n",
    "\n",
    "It is important to fit the vector on the train data only.  Then the vector is used to transform both the train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate the vectorizer and fit on the training set\n",
    "vect = CountVectorizer()\n",
    "vect.fit(x_train)\n",
    "\n",
    "# Use the trained to create a document-term matrix from train and test sets\n",
    "x_train_dtm = vect.transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "\n",
    "#source: https://www.kaggle.com/andreshg/nlp-glove-bert-tf-idf-lstm-explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "TF-IDF is also important for text analysis.  It has two components:\n",
    "\n",
    "- TF = \"Term Frequency\"\n",
    "- IDF = \"Inverse Document Frequency\"\n",
    "\n",
    "In a text database, the frequency of words could be inversely correlated to how important the word is.  Common words might not reveal much about patterns for classifications.  But some words that are used less frequently are key words in discovering patterns.   \n",
    "\n",
    "TF-IDF transformation provides word frequency scores for each word in the data, with less important common words having less significance than key words.  It acts to highlight interesting words in a message - the ones that might be frequent in a message, but not necessarily frequent across messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4457x6303 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 35945 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_transformer.fit(x_train_dtm)\n",
    "x_train_tfidf = tfidf_transformer.transform(x_train_dtm)\n",
    "\n",
    "x_train_tfidf\n",
    "\n",
    "#source: https://www.kaggle.com/andreshg/nlp-glove-bert-tf-idf-lstm-explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A row of the matrix will look like the output below.  0 means the word is not in the sms message for that row, and a 1 means that the word is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted = x_train_tfidf.todense()\n",
    "formatted[0]\n",
    "\n",
    "#source: https://stackoverflow.com/questions/15115765/how-to-access-sparse-matrix-elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now the data is processed/prepared and I am ready to train the model.  I use a Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(x_train_dtm, y_train)\n",
    "\n",
    "# Make class anf probability predictions\n",
    "y_pred_class = nb.predict(x_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the accuracy of the model?  Report your finding with corresponding tables/graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is: 0.9820627802690582\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[961,   4],\n",
       "       [ 16, 134]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy of the model is:\", metrics.accuracy_score(y_test, y_pred_class))\n",
    "print('')\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = (metrics.confusion_matrix(y_test, y_pred_class))\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model scores an accuracy of 0.982 which seems pretty successful.  The confusion matrix shows that 961 ham messages were classified correctly, while 4 spam messages were classified as ham.  Also, 134 spam messages were classified correctly while 16 ham messages were classified as spam.  These scores are pretty good overall!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the 5 most frequent words in each class, and their posterior probability generated by the model.\n",
    "\n",
    "I interpreted this to mean the 5 words from each class that occur in the most messages of that class, not neccesarily the words that occur most in the entire dataset.  Some words could occur mulitple times in a message, which could skew the results.  I more focus on which words occur in the most messages of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in Ham messages: 6057\n",
      "Unique words in Spam messages: 1951\n"
     ]
    }
   ],
   "source": [
    "# Get all the words in the Ham and Spam messages\n",
    "\n",
    "# Split messages into ham and spam\n",
    "ham_messages = []\n",
    "spam_messages = []\n",
    "\n",
    "for x in range(len(df['stemmed'])):\n",
    "    if df['b_target'][x] == 0:\n",
    "        ham_messages.append(df['stemmed'][x])\n",
    "    else:\n",
    "        spam_messages.append(df['stemmed'][x])\n",
    "        \n",
    "# Put words in lists for each class\n",
    "ham_words = []\n",
    "spam_words = []\n",
    "\n",
    "for message in ham_messages:\n",
    "    words = message.split()\n",
    "    for word in words:\n",
    "        ham_words.append(word)\n",
    "        \n",
    "for message in spam_messages:\n",
    "    words = message.split()\n",
    "    for word in words:\n",
    "        spam_words.append(word)\n",
    "        \n",
    "#Remove duplicates\n",
    "unique_ham_words = set(ham_words)\n",
    "unique_spam_words = set(spam_words)\n",
    "\n",
    "print(\"Unique words in Ham messages:\",len(unique_ham_words))\n",
    "print(\"Unique words in Spam messages:\",len(unique_spam_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6057/6057 [06:28<00:00, 15.59it/s]\n",
      "100%|██████████| 1951/1951 [01:13<00:00, 26.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# Initiate dictionaries \n",
    "ham_word_freq = {}\n",
    "spam_word_freq = {}\n",
    "\n",
    "# Go through unique words for ham\n",
    "for word in tqdm(list(unique_ham_words)):\n",
    "    # Loop through each row of the database\n",
    "    for x in range(len(df['stemmed'])):\n",
    "        # For the specific class\n",
    "        if df['b_target'][x] == 0:\n",
    "            # if the word isn't yet in the dicitonary, instantiate it\n",
    "            if word not in ham_word_freq:\n",
    "                ham_word_freq[word] = 0\n",
    "        \n",
    "            # If the word is in the message for that row, increment the counter\n",
    "            # We are counting how many messages (rows) that each word in ham occurs, and getting the most frequent\n",
    "            if word in df['stemmed'][x].split():\n",
    "                ham_word_freq[word] += 1\n",
    "    \n",
    "for word in tqdm(list(unique_spam_words)):\n",
    "    for x in range(len(df['stemmed'])):\n",
    "        if df['b_target'][x] == 1:\n",
    "            if word not in spam_word_freq:\n",
    "                spam_word_freq[word] = 0\n",
    "        \n",
    "            if word in df['stemmed'][x].split():\n",
    "                spam_word_freq[word] += 1        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 Ham Words: [('im', 412), ('go', 383), ('get', 337), ('come', 273), ('call', 265)]\n",
      "The top 5 Spam Words: [('call', 328), ('free', 169), ('txt', 142), ('text', 122), ('mobil', 118)]\n",
      "6057 unique words in ham.\n",
      "1951 unique words in spam.\n"
     ]
    }
   ],
   "source": [
    "# Sort the dictionary above.  The strucuture is word as the key and number of messages containing that word as the value\n",
    "# Sort by value\n",
    "sorted_ham_word_freq = {k: v for k, v in sorted(ham_word_freq.items(), key=lambda item: item[1], reverse=True)}\n",
    "# Save a list of the top 5 values.  These are the words that occur in the most rows of Ham\n",
    "top_5_ham = list(sorted_ham_word_freq.items())[0:5]\n",
    "\n",
    "sorted_spam_word_freq = {k: v for k, v in sorted(spam_word_freq.items(), key=lambda item: item[1], reverse=True)}\n",
    "top_5_spam = list(sorted_spam_word_freq.items())[0:5]\n",
    "\n",
    "print(\"The top 5 Ham Words:\",top_5_ham)\n",
    "print(\"The top 5 Spam Words:\",top_5_spam)\n",
    "\n",
    "print(len(list(sorted_ham_word_freq.items())),\"unique words in ham.\")\n",
    "print(len(list(sorted_spam_word_freq.items())),\"unique words in spam.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Odds Calculation\n",
    "\n",
    "Info Needed to Calculate Posterior:\n",
    "\n",
    "- Ham Messages: 4,825 \n",
    "- Spam Messages: 747\n",
    "- Total Messages: 5,572\n",
    "- % Ham: 86.59%\n",
    "- % Spam:13.41%\n",
    "\n",
    "\n",
    "Intuitively, posterior probability in this case answers the question: given one of the top 5 words, what is the probability that the message is Ham (or Spam in the next section).\n",
    "\n",
    "Posterior = (Likelihood * Prior) / Evidence\n",
    "\n",
    "Where in this case for example, \n",
    "\n",
    "- Posterior = P(Ham | Word) = The probability that a message is ham, given it contains the word\n",
    "- Likelihood = P(Word | Ham) = The probability that a message contains the word, given that the message is ham\n",
    "- Prior = P(Ham) = The probability that a message is Ham\n",
    "- Evidence = P(Word) = The probability of the word occuring in a message = Messages with the word / total messages\n",
    "\n",
    "\n",
    "#### Ham Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "The posterior for: im is: 0.9716981132075471\n",
      "########################\n",
      "The posterior for: go is: 0.911904761904762\n",
      "########################\n",
      "The posterior for: get is: 0.7985781990521327\n",
      "########################\n",
      "The posterior for: come is: 0.9820143884892087\n",
      "########################\n",
      "The posterior for: call is: 0.4468802698145026\n"
     ]
    }
   ],
   "source": [
    "ham_messages = 4825\n",
    "spam_messages = 747\n",
    "total_messages = ham_messages + spam_messages\n",
    "\n",
    "# Loop through each word, frequency pairing\n",
    "for tup in top_5_ham:    \n",
    "    # Save the word \n",
    "    word = tup[0]\n",
    "    \n",
    "    print(\"########################\")\n",
    "    \n",
    "    ##################### Likelihood ##########################\n",
    "    # count how many ham messages contain the word / total ham messages\n",
    "    # how many ham messages is equal to the second value in the tuple, but it is good to calculate again to understand the intuition\n",
    "    class_messages_with_word = 0\n",
    "\n",
    "    # Go through every row of the df\n",
    "    for x in range(len(df['stemmed'])):\n",
    "        # If the row is labeled as ham\n",
    "        if df['b_target'][x] == 0:\n",
    "            # and if the word is in the row's message\n",
    "            if word in df['stemmed'][x].split():\n",
    "                # Count it\n",
    "                class_messages_with_word += 1\n",
    "                    \n",
    "    likelihood = class_messages_with_word / ham_messages \n",
    "    \n",
    "    #################### Prior ##################################\n",
    "    # Ham Messages / Total Messages\n",
    "    prior = ham_messages / total_messages\n",
    "    \n",
    "    ###################### Evidence ############################\n",
    "    # Messages with the word / total messages\n",
    "    all_messages_with_word = 0\n",
    "\n",
    "    # Similar loop to above, except counting all times the word occurs in a message, not just ham messages\n",
    "    for x in range(len(df['stemmed'])):\n",
    "        if word in df['stemmed'][x].split():\n",
    "            all_messages_with_word += 1\n",
    "        \n",
    "    evidence = all_messages_with_word / total_messages\n",
    "                \n",
    "                \n",
    "    posterior = likelihood * prior / evidence\n",
    "    \n",
    "    print(\"The posterior for:\",word,\"is:\",posterior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spam Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "The posterior for: call is: 0.5531197301854975\n",
      "########################\n",
      "The posterior for: free is: 0.7444933920704847\n",
      "########################\n",
      "The posterior for: txt is: 0.922077922077922\n",
      "########################\n",
      "The posterior for: text is: 0.5951219512195122\n",
      "########################\n",
      "The posterior for: mobil is: 0.887218045112782\n"
     ]
    }
   ],
   "source": [
    "for tup in top_5_spam:    \n",
    "    word = tup[0]\n",
    "    \n",
    "    print(\"########################\")\n",
    "    \n",
    "    ##################### Likelihood ##########################\n",
    "    # count how many ham messages contain the word / total ham messages\n",
    "    class_messages_with_word = 0\n",
    "\n",
    "    for x in range(len(df['stemmed'])):\n",
    "        if df['b_target'][x] == 1:\n",
    "            if word in df['stemmed'][x].split():\n",
    "                class_messages_with_word += 1\n",
    "                    \n",
    "    likelihood = class_messages_with_word / spam_messages \n",
    "    \n",
    "    #################### Prior ##################################\n",
    "    # Ham Messages / Total Messages\n",
    "    prior = spam_messages / total_messages\n",
    "    \n",
    "    ###################### Evidence ############################\n",
    "    # Messages with the word / total messages\n",
    "    all_messages_with_word = 0\n",
    "\n",
    "    for x in range(len(df['stemmed'])):\n",
    "        if word in df['stemmed'][x].split():\n",
    "            all_messages_with_word += 1\n",
    "        \n",
    "    evidence = all_messages_with_word / total_messages\n",
    "                \n",
    "                \n",
    "    posterior = likelihood * prior / evidence\n",
    "    \n",
    "    print(\"The posterior for:\",word,\"is:\",posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Analysis\n",
    "\n",
    "- Messages with the words \"im\", \"go\", or \"come\" have > .90 probability of being Ham\n",
    "- Messages with the word \"txt\" has a 0.92 probability of being spam.  But the word 'text' has a probability of 0.59.  This indidcates that it is more likely to be abbreviated in spam messages.  \n",
    "- The word 'call' occurs frequently in both ham and spam messages, making it a lower probability than other words which appear frequently only in one class.\n",
    "- Even though these words seem common, a few give really good insight into whether a message is spam or ham.  This is surprising to me but demonstrates how effective Naive Bayes classifiers can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you improve the model performance?\n",
    "\n",
    "- More data\n",
    "- More balanced data\n",
    "- Perhaps using lemmatization instead of stemming would improve the model.\n",
    "- Increasing the n-grams to give the model context for strings of words.  This added context might be helpful in classifying ham vs spam because it would take into account word combinations.\n",
    "- Add more words to the stop list\n",
    "- Also, feature engineering might help improve the model.  In my EDA, I saw that spam messages were much longer on average than ham messages.  Engineering a 'message_length' feature and inputting it into the model might improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the data set is bigger, do you think the accuracy increases? Discuss.\n",
    "\n",
    "In general, I think a bigger dataset can always contribute to the model's accuracy.  There is just more input to train on.  I especially think more data would help if the additional data helped shift the balance of spam and ham.  Right now, the dataset inlcudes many more ham datapoints than spam.  Also, if more features were added, I think that would help the model's accuracy significantly. For example, if a feature was added that indicated whether the message came from someone in the recipient's contact list, I would guess that it would be easier to classify ham vs. spam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
